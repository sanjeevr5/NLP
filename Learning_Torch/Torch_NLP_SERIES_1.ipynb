{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch_NLP_SERIES_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZjCATsIfrz0R6oZxzjEnn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeevr5/NLP/blob/main/Torch_NLP_SERIES_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udO_Cj5e9Q9k"
      },
      "source": [
        "# 1. Multiclass Classification With TorchText & TabularDataset\n",
        "\n",
        "\n",
        "- Architecture Used : Simple RNN\n",
        "- Referring : https://github.com/bentrevett/pytorch-sentiment-analysis\n",
        "- Field : https://github.com/pytorch/text/blob/master/torchtext/data/field.py\n",
        "- Dataset : https://github.com/mhjabreel/CharCnn_Keras/blob/master/data/ag_news_csv\n",
        "- Label : {1: 'WORLD', 2: 'SPORTS', 3: 'BIZ', 4: 'TECH'}\n",
        "- Custom embeddings used\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqqUhxZt85v8"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext.data import TabularDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "SEED = 34\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "NEWS = data.Field(tokenize = 'spacy', lower = True)\n",
        "CLASS = data.LabelField(dtype = torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxzjoUOTL9kV"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Mr8zHcO5-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e84204-c88b-4480-bd73-52bd6f0d7b4d"
      },
      "source": [
        "!head -5 train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\"\n",
            "\"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\"\n",
            "\"3\",\"Oil and Economy Cloud Stocks' Outlook (Reuters)\",\"Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\"\n",
            "\"3\",\"Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)\",\"Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\"\n",
            "\"3\",\"Oil prices soar to all-time record, posing new menace to US economy (AFP)\",\"AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APB93k0Ji8Sg"
      },
      "source": [
        "!sed -i '1s/^/\"cat\",\"title\",\"news\"\\n/' ./train.csv\n",
        "!sed -i '1s/^/\"cat\",\"title\",\"news\"\\n/' ./test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4pvg0v9PeYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05590f24-6d06-4647-ca54-aa766d887e5e"
      },
      "source": [
        "!head -5 train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"cat\",\"title\",\"news\"\n",
            "\"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\"\n",
            "\"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\"\n",
            "\"3\",\"Oil and Economy Cloud Stocks' Outlook (Reuters)\",\"Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\"\n",
            "\"3\",\"Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)\",\"Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ6wZVsuOqwq"
      },
      "source": [
        "field_names = [('cat', CLASS), ('title', NEWS), (2, None)]\n",
        "train, test = TabularDataset.splits('/content/', train = 'train.csv', test = 'test.csv', format = 'CSV', fields = field_names, skip_header = True)\n",
        "#train, val = train.split(stratified = True, strata_field = 'cat', random_state = SEED) # WORKS ON TENSOR BASIS ONLY (NOT USING SPLITS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j041cKt1PuoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48e3018-67be-4dba-cc0f-8ecc1d5af593"
      },
      "source": [
        "NEWS.build_vocab(train, max_size = 15000) #<UNK> and <PAD> tokens are attached hence 15k+2\n",
        "CLASS.build_vocab(train)\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(NEWS.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(CLASS.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 15002\n",
            "Unique tokens in LABEL vocabulary: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8whsHpYdgNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbf3b83-a19f-4ea8-fdad-d8769ce2acd2"
      },
      "source": [
        "print(NEWS.vocab.freqs.most_common(20)) #most common items\n",
        "print(NEWS.vocab.itos[:10])#integer to string\n",
        "print(CLASS.vocab.stoi) #string to integer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('to', 23896), ('in', 17660), ('(', 17132), (')', 17130), (',', 16321), ('-', 13503), ('#', 12950), ('for', 12435), (':', 9629), ('on', 9584), ('of', 9078), (';', 7780), ('ap', 7777), ('the', 6415), ('39;s', 6160), ('a', 4915), (\"'\", 4328), ('reuters', 4261), ('at', 4231), ('with', 4088)]\n",
            "['<unk>', '<pad>', 'to', 'in', '(', ')', ',', '-', '#', 'for']\n",
            "defaultdict(<function _default_unk_index at 0x7f51ff9a21e0>, {'1': 0, '2': 1, '3': 2, '4': 3})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxcHXCVJP8bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57fb0275-c980-4ae3-ba4b-dd4ca02e001e"
      },
      "source": [
        "print(f'Number of training examples: {len(train)}')\n",
        "print(f'Number of testing examples: {len(test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 120000\n",
            "Number of testing examples: 7600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUkVb63VP9_R"
      },
      "source": [
        "- We will try to predict stuffs only using the title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LosiyudQOIM"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, test), sort = True, sort_key = lambda x : len(x.title), sort_within_batch = True, \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrxcK62XDQCk"
      },
      "source": [
        "## 1 Simple RNN Training\n",
        "\n",
        "- Takes a hidden layer representation which is equal to the number of hidden units\n",
        "- The input is the embedding vector\n",
        "- Initially the hidden vector will consist only of zeros\n",
        "\n",
        "> In PyTorch the batch becomes the second dimension and hence the input to the RNN is [sentence_length, batch_size, one_hot vector] <br>\n",
        "> Now, the output of the embedding layer is [sentence_len, batch_size, embedding_vector]<br>\n",
        "> RNN performs tanh(embedding_vector, hidden_state)<br>\n",
        "> The RNN will output two vectors one is the output vector and the other is the hidden state <br>\n",
        "> The hidden vector is of the shape : [1, batch_size, hidden_size] interpret as one hidden vector per review in a batch <br>\n",
        "> The output vector is of the shape : [sentence_len, batch_size, hidden_size] interpret as one hidden vector per review in a batch <br>\n",
        "> The difference between the output vector and the hidden vector is that it has the hidden state for every time_step and the hidden vector consists of the hidden state of the final time step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-oXTy4zj73K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf21103c-85ea-49b3-f4c2-03f7e9911b3f"
      },
      "source": [
        "class RNN_Single(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, embed_size, hidden_state_size, classes):\n",
        "    super(RNN_Single, self).__init__()\n",
        "    self.embed = nn.Embedding(input_dim, embed_size)\n",
        "    self.rnn = nn.RNN(embed_size, hidden_state_size)\n",
        "    self.fc = nn.Linear(hidden_state_size, classes)\n",
        "\n",
        "  def forward(self, input_batch):\n",
        "\n",
        "    embedding_batch = self.embed(input_batch)\n",
        "    #embedding batch is [batch_size, sentence_len, embeddings]\n",
        "    #xxxxxxxxxxxxxxxx Note : that since we are using TorchText there is no need of padding or explicitly mentioning the sentence lengths\n",
        "    # The process of using Embeddings and LSTM is much more different when used it explicitly without the TorchText xxxxxxxxxxxxxxxxxxx\n",
        "    output, hidden = self.rnn(embedding_batch)\n",
        "    hidden = hidden.squeeze(0)\n",
        "    assert torch.equal(output[-1,:,:], hidden) # Comparing the last time step output vector to the hidden vector and this should be equal\n",
        "    return self.fc(hidden)\n",
        "\n",
        "INPUT_DIM = len(NEWS.vocab)\n",
        "EMBED_DIM = 128\n",
        "HIDDEN_UNITS = 512\n",
        "CLASSES = 4\n",
        "\n",
        "model = RNN_Single(INPUT_DIM, EMBED_DIM, HIDDEN_UNITS, CLASSES)\n",
        "\n",
        "print('The number of trainable parameters are :', sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of trainable parameters are : 2251012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTbUhF1AN6Ev"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr = 1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJGCDJrQQ1tH"
      },
      "source": [
        "\"\n",
        "You may recall when initializing the LABEL field, we set dtype=torch.float. This is because TorchText sets tensors to be LongTensors by default, however our criterion expects both inputs to be FloatTensors. Setting the dtype to be torch.float, did this for us. The alternative method of doing this would be to do the conversion inside the train function by passing batch.label.float() instad of batch.label to the criterion.\n",
        "\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG5eR7z9O7zn"
      },
      "source": [
        "def accuracy(preds, true):\n",
        "  _, index = torch.max(preds, dim = 1)\n",
        "  return (index == true).sum().float() / len(preds)\n",
        "\n",
        "def train_m(model, iterator, optimizer, l):\n",
        "  e_loss = 0\n",
        "  e_acc = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch in iterator:\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(batch.title)# Call using the column name\n",
        "    acc = accuracy(preds,  batch.cat)\n",
        "    loss = l(preds.squeeze(1), batch.cat.long())\n",
        "    acc = accuracy(preds,  batch.cat)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    e_loss += loss.item()\n",
        "    e_acc += acc.item()\n",
        "  return e_loss/len(iterator), e_acc/len(iterator)\n",
        "\n",
        "def evaluate_m(model, iterator, l):\n",
        "  e_loss = 0\n",
        "  e_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      preds = model(batch.title)\n",
        "      loss = l(preds.squeeze(1), batch.cat.long())\n",
        "      acc = accuracy(preds,  batch.cat)\n",
        "      e_loss += loss.item()\n",
        "      e_acc += acc.item()\n",
        "  return e_loss/len(iterator), e_acc/len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA7r0h0qTKKK"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVeQd3uJYIpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281ee7e4-c784-415d-bf65-6eecceaa6bab"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train_m(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_m(model, test_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} / {N_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 / 5 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 1.366 | Train Acc: 30.60%\n",
            "\t Val. Loss: 1.352 |  Val. Acc: 32.08%\n",
            "Epoch: 02 / 5 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 1.336 | Train Acc: 34.98%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 35.63%\n",
            "Epoch: 03 / 5 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 1.315 | Train Acc: 37.55%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 37.89%\n",
            "Epoch: 04 / 5 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 1.297 | Train Acc: 39.30%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 39.33%\n",
            "Epoch: 05 / 5 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 1.279 | Train Acc: 40.80%\n",
            "\t Val. Loss: 1.271 |  Val. Acc: 40.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6f0XLOe_vZY"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vx61_Af_2UE"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "CLASS_DICT = {0: 'WORLD', 1: 'SPORTS', 2: 'BIZ', 3: 'TECH'}\n",
        "def predict(sentence):\n",
        "  model.eval()\n",
        "  tokens = [word.text for word in nlp.tokenizer(sentence)]\n",
        "  indices = [NEWS.vocab.stoi[word] for word in tokens]\n",
        "  input_tensor = torch.LongTensor(indices).to(device)\n",
        "  input_tensor = input_tensor.unsqueeze(1)\n",
        "  #length_tensor = torch.LongTensor([len(indices)])\n",
        "  prediction = torch.sigmoid(model(input_tensor))\n",
        "  _, index = torch.max(prediction, dim = 1)\n",
        "  return CLASS_DICT[index.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0eHS8ld3CKGm",
        "outputId": "3fb07652-d7dd-4e8e-fecb-7558cfb8dfaf"
      },
      "source": [
        "predict('Oil prices are up!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TECH'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcPnE3ZI_WT_"
      },
      "source": [
        "<b> Points learnt </b>\n",
        "\n",
        "- Labels should be always of long type for CrossEntropy & float for BCEWithLogitsLoss\n",
        "- While using data.BucketIterator.splits make sure sort is set to False https://github.com/pytorch/text/issues/474"
      ]
    }
  ]
}
