{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch_NLP_SERIES_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHon1AHKl+sCrsnP/KB64Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeevr5/NLP/blob/main/Torch_NLP_SERIES_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPD1roe8Ec8a"
      },
      "source": [
        "# 2. Multi-Class Classification (Predicting News Category) Without TorchText\n",
        "\n",
        "\n",
        "- Architecture Used : Stacked Bi-Directional LSTM With BPEmb embeddings(https://github.com/bheinzerling/bpemb)\n",
        "- Referring : https://github.com/bentrevett/pytorch-sentiment-analysis\n",
        "- Field : https://github.com/pytorch/text/blob/master/torchtext/data/field.py\n",
        "- Dataset : https://github.com/mhjabreel/CharCnn_Keras/blob/master/data/ag_news_csv\n",
        "- Label Reference : {0: 'WORLD', 1: 'SPORTS', 2: 'BIZ', 3: 'TECH'}\n",
        "- **Concepts covered : packed_sequences, custom data iterator and data loader for variable length data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtdxOPlKF6NH"
      },
      "source": [
        "%%capture\n",
        "!pip install bpemb\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK7V3kpLEaJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54430ff-cf97-4d40-8829-1737dc43081a"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "from bpemb import BPEmb\n",
        "bpemb_en = BPEmb(lang=\"en\", dim=300, vs = 10000) #vs will be the voacb size\n",
        "print(bpemb_en.vectors.shape)\n",
        "embed_matrix = bpemb_en.vectors\n",
        "SEED = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 570003.42B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d300.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11189884/11189884 [00:01<00:00, 6605870.04B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(10000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m3cCOpEFRPw",
        "outputId": "61052245-a974-47a5-9659-358fed3579b1"
      },
      "source": [
        "!head -5 train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\"\n",
            "\"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\"\n",
            "\"3\",\"Oil and Economy Cloud Stocks' Outlook (Reuters)\",\"Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\"\n",
            "\"3\",\"Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)\",\"Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\"\n",
            "\"3\",\"Oil prices soar to all-time record, posing new menace to US economy (AFP)\",\"AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS8_3P4pFTYH"
      },
      "source": [
        "!sed -i '1s/^/\"cat\",\"title\",\"news\"\\n/' ./train.csv\n",
        "!sed -i '1s/^/\"cat\",\"title\",\"news\"\\n/' ./test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnYU7t3xFT4P",
        "outputId": "6c201bc1-fe14-4d64-a0bd-41bdde01ec13"
      },
      "source": [
        "!head -5 test.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"cat\",\"title\",\"news\"\n",
            "\"3\",\"Fears for T N pension after talks\",\"Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"\n",
            "\"4\",\"The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com)\",\"SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.\"\n",
            "\"4\",\"Ky. Company Wins Grant to Study Peptides (AP)\",\"AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\"\n",
            "\"4\",\"Prediction Unit Helps Forecast Wildfires (AP)\",\"AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgrmFaXCUkCu"
      },
      "source": [
        "train_data = pd.read_csv('./train.csv', sep = ',')\n",
        "test_data = pd.read_csv('./test.csv', sep = ',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "A77MAKPkVfy0",
        "outputId": "5a49968f-f473-4e28-c276-fb7765540fab"
      },
      "source": [
        "print(f'Train data shape : {train_data.shape}')\n",
        "print(f'Test data shape : {test_data.shape}')\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape : (120000, 3)\n",
            "Test data shape : (7600, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>title</th>\n",
              "      <th>news</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cat  ...                                               news\n",
              "0    3  ...  Reuters - Short-sellers, Wall Street's dwindli...\n",
              "1    3  ...  Reuters - Private investment firm Carlyle Grou...\n",
              "2    3  ...  Reuters - Soaring crude prices plus worries\\ab...\n",
              "3    3  ...  Reuters - Authorities have halted oil export\\f...\n",
              "4    3  ...  AFP - Tearaway world oil prices, toppling reco...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHAkyWuVWVt7"
      },
      "source": [
        "train_text = train_data.title.map(lambda x : x.split(' (')[0].lower() )\n",
        "test_text = test_data.title.map(lambda x : x.split(' (')[0].lower() )\n",
        "train_label = train_data.cat.values - 1 # labels should start from 0\n",
        "test_label = test_data.cat.values - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZvF9wgwW8OC",
        "outputId": "6f35ddad-2439-40f3-c272-c87334b2cdef"
      },
      "source": [
        "print('The train text shape : ', train_text.shape)\n",
        "print('The test text shape : ', test_text.shape)\n",
        "print('The train label shape : ', train_label.shape)\n",
        "print('The train text shape : ', test_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train text shape :  (120000,)\n",
            "The test text shape :  (7600,)\n",
            "The train label shape :  (120000,)\n",
            "The train text shape :  (7600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzjNfH5FdGRj"
      },
      "source": [
        "train_encoded = [torch.tensor(bpemb_en.encode_ids(item)) for item in train_text]\n",
        "test_encoded = [torch.tensor(bpemb_en.encode_ids(item)) for item in test_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VVppCPxii82"
      },
      "source": [
        "class Data_Iterator(data.Dataset):\n",
        "\n",
        "  def __init__(self, text, label):\n",
        "    super(Data_Iterator, self).__init__()\n",
        "    assert len(text) == len(label)\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.text[index], self.label[index]\n",
        "\n",
        "train_data = Data_Iterator(train_encoded, train_label)\n",
        "test_data = Data_Iterator(test_encoded, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzV4WCM22tzg"
      },
      "source": [
        "def packed(batch):\n",
        "  \"\"\"\n",
        "  https://www.codefull.net/2018/11/use-pytorchs-dataloader-with-variable-length-sequences-for-lstm-gru/\n",
        "  how to use dataloader with variable length sequences : LIFE SAVER!!!\n",
        "  The real reason for going with dataloader is parallelization though it is not used here :)\n",
        "  \"\"\"\n",
        "  sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
        "  text_seq = [i[0] for i in sorted_batch]\n",
        "  text_seq = pack_sequence(text_seq, enforce_sorted=False).to(device)\n",
        "  leng = torch.LongTensor([len(i[0]) for i in sorted_batch]).to(device)\n",
        "  target = torch.LongTensor([i[1] for i in sorted_batch]).to(device)\n",
        "  return text_seq, leng, target\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=False, collate_fn= packed) \n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False, collate_fn = packed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtvy6JZ8s_6y"
      },
      "source": [
        "<b> Implementation Details </b>\n",
        "\n",
        "- We are going to use bi-directional LSTM stacked on top of one another\n",
        "- For RNN we get output, hidden but for LSTM we get output, (hidden, cell) since cell state is known to solve the problem of vanishing gradients\n",
        "- Just like RNN hidden and cell state consits of the vector representations of size hidden units of the last timestep\n",
        "- Since, we are using Bi representation we will get the hidden and cell state vector with 2 * hidden units one for the forward pass and the next for the backward pass [forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]\n",
        "- The final hidden or cell state is of the shape [2 * n_layers, batch_size, hidden_dims] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvvMSSc2PZjl",
        "outputId": "5cd17649-e4dd-4ca7-f810-a57474e3928c"
      },
      "source": [
        "class Seq_Model(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_size, lstm_hidden_units, n_layers, n_classes, bi_d = True, drop_rate = 0.3):\n",
        "    super(Seq_Model, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding.from_pretrained(torch.as_tensor(embed_matrix))\n",
        "    self.seq = nn.LSTM(embedding_size, lstm_hidden_units, num_layers = n_layers, bidirectional = bi_d, dropout = drop_rate)\n",
        "    self.fc = nn.Linear(2 * lstm_hidden_units, n_classes)\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "  def forward(self, input_batch):\n",
        "    \"\"\"\n",
        "      ref : \"https://discuss.pytorch.org/t/how-to-use-pack-sequence-if-we-are-going-to-use-word-embedding-and-bilstm/28184/4\"\n",
        "      the problem is packed sequences can be directly fed to LSTM/RNN but not to the embedding layer!\n",
        "    \"\"\"\n",
        "    #input_batch = [sent len, batch size]\n",
        "\n",
        "    #embed = self.dropout(simple_elementwise_apply(self.embedding, input_batch))#self.embedding(simple_elementwise_applyinput_batch))\n",
        "    #embed = [seq_len, batch_size, embed_dim]\n",
        "    embed = simple_elementwise_apply(self.embedding, input_batch)\n",
        "    #packed_embedded = nn.utils.rnn.pack_padded_sequence(embed, seq_lens)\n",
        "    packed_output, (hidden, cell) = self.seq(embed)\n",
        "    #output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "    # Concatenate the forward and backward hidden state\n",
        "    return self.fc(hidden)\n",
        "\n",
        "def simple_elementwise_apply(fn, packed_sequence):\n",
        "    \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
        "    return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)\n",
        "\n",
        "model = Seq_Model(300, 512, 2, 4)\n",
        "print('The number of trainable parameters are :', sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of trainable parameters are : 9637892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db01cxWePbz5"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTdXZctRPd-8"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGksR5e6Pf7t"
      },
      "source": [
        "def accuracy(preds, true):\n",
        "  _, index = torch.max(preds, dim = 1)\n",
        "  return (index == true).sum().float() / len(preds)\n",
        "\n",
        "def train_m(model, iterator, optimizer, l):\n",
        "  e_loss = 0\n",
        "  e_acc = 0\n",
        "  model.train()\n",
        "\n",
        "  for inputs, labels in iterator:\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(inputs)\n",
        "    acc = accuracy(preds,  labels)\n",
        "    loss = l(preds.squeeze(1), labels.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    e_loss += loss.item()\n",
        "    e_acc += acc.item()\n",
        "  return e_loss/len(iterator), e_acc/len(iterator)\n",
        "\n",
        "def evaluate_m(model, iterator, l):\n",
        "  e_loss = 0\n",
        "  e_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in iterator:\n",
        "      preds = model(inputs)\n",
        "      loss = l(preds.squeeze(1), labels.long())\n",
        "      acc = accuracy(preds,  labels)\n",
        "      e_loss += loss.item()\n",
        "      e_acc += acc.item()\n",
        "  return e_loss/len(iterator), e_acc/len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIWB4h9YPljr",
        "outputId": "c8d612ab-0a0e-4591-fc73-220b93abc36d"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train_m(model, trainloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_m(model, testloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} / {N_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 / 5 | Epoch Time: 0m 58s\n",
            "\tTrain Loss: 1.133 | Train Acc: 54.76%\n",
            "\t Val. Loss: 1.119 |  Val. Acc: 54.29%\n",
            "Epoch: 02 / 5 | Epoch Time: 0m 58s\n",
            "\tTrain Loss: 1.083 | Train Acc: 56.85%\n",
            "\t Val. Loss: 1.112 |  Val. Acc: 55.00%\n",
            "Epoch: 03 / 5 | Epoch Time: 0m 59s\n",
            "\tTrain Loss: 1.036 | Train Acc: 58.96%\n",
            "\t Val. Loss: 1.128 |  Val. Acc: 54.88%\n",
            "Epoch: 04 / 5 | Epoch Time: 0m 59s\n",
            "\tTrain Loss: 0.972 | Train Acc: 61.52%\n",
            "\t Val. Loss: 1.187 |  Val. Acc: 54.56%\n",
            "Epoch: 05 / 5 | Epoch Time: 0m 59s\n",
            "\tTrain Loss: 0.891 | Train Acc: 64.43%\n",
            "\t Val. Loss: 1.272 |  Val. Acc: 53.32%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRVg0j-xLPY9"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKdX3cSsLO1m"
      },
      "source": [
        "LABEL_DICT =  {0: 'WORLD', 1: 'SPORTS', 2: 'BIZ', 3: 'TECH'}\n",
        "\n",
        "def predict(sentence):\n",
        "  input_tensor = pack_sequence([torch.LongTensor(bpemb_en.encode_ids(sentence))], enforce_sorted=False).to(device)\n",
        "  pred = model(input_tensor)\n",
        "  _, index = torch.max(pred, dim = 1)\n",
        "  return LABEL_DICT[index.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PI6qKj4RTYEJ",
        "outputId": "fefe3d93-2c08-4a9a-92d9-7a76bab4f6da"
      },
      "source": [
        "predict('Oil prices are up!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'BIZ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}
